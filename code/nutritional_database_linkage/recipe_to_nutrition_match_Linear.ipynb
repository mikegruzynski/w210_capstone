{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height has been deprecated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "import numpy as np\n",
    "import re \n",
    "\n",
    "master_df = pd.read_csv(\"C:/Users/mgruz/Desktop/w210/data/nutrient/compiled/nutrition_master_df.csv\")\n",
    "\n",
    "with open('C:/Users/mgruz/Desktop/w210/data/recipe/NDB_NO_tag_dict.json') as f:\n",
    "    NDB_NO_tag_dict = json.load(f)\n",
    "    \n",
    "with open('C:/Users/mgruz/Desktop/w210/data/recipe/recipe_all.json') as f:\n",
    "    original_recipe = json.load(f)\n",
    "    \n",
    "with open('C:/Users/mgruz/Desktop/w210/data/recipe/recipe_clean.json') as f:\n",
    "    recipe_clean = json.load(f)\n",
    "    \n",
    "with open('C:/Users/mgruz/Desktop/w210/data/recipe/recipe_clean_anna.json') as f:\n",
    "    recipe_clean_anna = json.load(f)\n",
    "    \n",
    "with open('C:/Users/mgruz/Desktop/w210/data/recipe/recipe_clean_maura_v1.json') as f:\n",
    "    recipe_clean_maura_v1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duck fat  04542\n",
      "ground all spice 02001\n",
      "sweet dessert wine 14057\n",
      "canned pumpkin 11424\n",
      "peppercorns 02030\n",
      "white bread dough 18014\n",
      "dried red pepper flakes '02031'\n",
      "boneless leg of lamb butterflied 17300\n",
      "hot cherry peppers peppers\n",
      "blanched whole almonds 12062\n",
      "shredded red cabbage 11112\n",
      "2-percent mozzarella cheese 01029\n",
      "for purging exi\n",
      "baby bok choy 11116\n",
      "FAIL pkg martha white chocolate chip muffin mix\n",
      "reduced-sodium chicken broth 06970\n",
      "mini rigatoni 20420\n",
      "dried hot red pepper 02031\n",
      "raspberry 09302\n",
      "bunch watercress stems 11591\n",
      "english muffins 18639\n",
      "five-spice powder 02031\n",
      "fresh rosemary 02063\n",
      "toasted pecans coarsely 12142\n",
      "small-diced carrots 11124\n",
      "dark raisins 09299\n",
      "red radishes 11429\n"
     ]
    }
   ],
   "source": [
    "for i in NDB_NO_tag_dict.keys():\n",
    "    \n",
    "    try:\n",
    "        if '\"' not in NDB_NO_tag_dict[i] and NDB_NO_tag_dict[i] != 'np.nan' and NDB_NO_tag_dict[i] != '':\n",
    "            print i, NDB_NO_tag_dict[i]\n",
    "            \n",
    "            if '\"' not in NDB_NO_tag_dict[i]:\n",
    "                NDB_NO_tag_dict[i] = '\"{}\"'.format(NDB_NO_tag_dict[i].strip(\"\\t\"))\n",
    "    except:\n",
    "        print 'FAIL', i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_clean_ALL = {}\n",
    "\n",
    "for key in recipe_clean.keys():\n",
    "    recipe_clean_ALL[key] = recipe_clean[key]\n",
    "    recipe_clean_ALL[key]['NDB_NO_tags'] = []\n",
    "    \n",
    "for key in recipe_clean_anna.keys():\n",
    "    recipe_clean_ALL[key] = recipe_clean_anna[key]\n",
    "    recipe_clean_ALL[key]['NDB_NO_tags'] = []\n",
    "    \n",
    "for key in recipe_clean_maura_v1.keys():\n",
    "    recipe_clean_ALL[key] = recipe_clean_maura_v1[key]\n",
    "    recipe_clean_ALL[key]['NDB_NO_tags'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itr = 0\n",
    "\n",
    "original_text = []\n",
    "nbd_tag = []\n",
    "for key in recipe_clean_ALL.keys():\n",
    "    \n",
    "    for i in recipe_clean_ALL[key]['ingredients']:\n",
    "        \n",
    "        ingredient_split = i.split(\" \")\n",
    "        ingredient_split = filter(None, ingredient_split)\n",
    "        ingredient_new = \" \".join(ingredient_split[2:])\n",
    "        \n",
    "        try:\n",
    "            recipe_clean_ALL[key]['NDB_NO_tags'].append(NDB_NO_tag_dict[ingredient_new])\n",
    "        except:\n",
    "            recipe_clean_ALL[key]['NDB_NO_tags'].append('')\n",
    "    \n",
    "    \n",
    "    if len(recipe_clean_ALL[key]['NDB_NO_tags']) == len(original_recipe[key]['ingredients']):       \n",
    "        itr_2 = 0\n",
    "        while itr_2 < len(recipe_clean_ALL[key]['NDB_NO_tags']):\n",
    "            original_text.append(original_recipe[key]['ingredients'][itr_2])\n",
    "            nbd_tag.append(recipe_clean_ALL[key]['NDB_NO_tags'][itr_2])\n",
    "            itr_2 += 1\n",
    "        \n",
    "        itr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units_of_food_recipe = [\"cups\", \"cup\", \"c.\", \"c\", \n",
    "                        \"fl. oz.\", \"fl oz\", \"fluid ounce\", \"fluid ounces\",\n",
    "                        \"gal\", \"gal.\", \"gallon\", \"gallons\",\n",
    "                        \"oz\", \"oz.\", \"ounce\", \"ounces\", \"ouncs\",\n",
    "                        \"pt\", \"pt.\", \"pint\", \"pints\",\n",
    "                        \"lb\", \"lb.\", \"pound\", \"pounds\",\n",
    "                        \"qt\", \"qt.\", \"qts\", \"qts.\", \"quart\", \"quarts\",\n",
    "                        \"tbsp.\", \"tbsp\", \"T\", \"T.\", \"tablespoon\", \"tablespoons\", \"tbs.\", \"tbs\",\n",
    "                        \"tsp.\", \"tsp\", \"t\", \"t.\", \"teaspoon\", \"teaspoons\",\n",
    "                        \"g\", \"g.\", \"gr\", \"gr.\", \"gram\", \"grams\", \"gramme\", \"grammes\",\n",
    "                        \"kg\", \"kg.\", \"kilogram\", \"kilograms\", \"kilogramme\", \"kilogrammes\",\n",
    "                        \"l\", \"l.\", \"liter\", \"liters\", \"litre\", \"litres\",\n",
    "                        \"mg\", \"mg.\", \"milligram\", \"milligrams\", \"milligramme\", \"milligrammes\",\n",
    "                        \"ml\", \"ml.\", \"milliliter\", \"milliliters\", \"millilitre\", \"millilitres\",\n",
    "                        \"pinch\", \"pinches\", \"dash\", \"dashes\", \"touch\", \"touches\", \"handful\", \"handfuls\",\n",
    "                        \"stick\", \"sticks\",\n",
    "                        \"cans\", \"can\",\n",
    "                        \"to taste\",\n",
    "                        \"scoop\", \"scoops\",\n",
    "                        \"dollop\", \"dollops\",\n",
    "                        \"sprig\", \"sprigs\",\n",
    "                       \"recipe\",\n",
    "                       \"garnish\", \"garnished\",\n",
    "                       \"sprinkle\", \"spinkled\",\n",
    "                       \"slices\",\n",
    "                       \"serving\", \"servings\",\n",
    "                       \"ribs\", \"rib\", \"stalk\",\n",
    "                       \"inch\", \"inches\", \"in.\",\n",
    "                       \"drizzle\", \"drizzled\",\n",
    "                       \"to taste\"]\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_original_text = []\n",
    "for original in original_text:\n",
    "    original = re.sub(r'\\s*(\\d+|[./+*-])', '', original)\n",
    "    original_split = original.split(\" \")\n",
    "    remove_list = list(set(original).symmetric_difference(units_of_food_recipe))\n",
    "    \n",
    "    for i in units_of_food_recipe:\n",
    "        if i in original_split:\n",
    "          original_split.remove(i)\n",
    "    \n",
    "    original_split = filter(None, original_split)\n",
    "    new = \" \".join(original_split)\n",
    "    \n",
    "    \n",
    "    filtered_original_text.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgruz\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\mgruz\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\mgruz\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "nlp_df = pd.DataFrame([original_text, filtered_original_text, nbd_tag]).transpose()\n",
    "nlp_df.columns = ['original_text', 'filtered_text','NBD_tag']\n",
    "nlp_df = nlp_df[nlp_df['NBD_tag'] != 'np.nan']\n",
    "nlp_df['NBD_tag'].replace('', np.nan, inplace=True)\n",
    "nlp_df.dropna(subset=['NBD_tag'], inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(nlp_df['filtered_text'], nlp_df['NBD_tag'], random_state=2, train_size=0.7)\n",
    "X_train_use, X_dev, y_train_use, y_dev = train_test_split(X_train, y_train, random_state=2, train_size=0.8)\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vecorized_train = vectorizer.fit_transform(X_train_use)\n",
    "vecorized_test = vectorizer.transform(X_test)\n",
    "vecorized_dev = vectorizer.transform(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_values = []\n",
    "knn_results = []\n",
    "for i in range(1, 100):\n",
    "    knn_f1 = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_f1.fit(vecorized_train, y_train_use)\n",
    "    knn_f1_prediction = knn_f1.predict(vecorized_dev)\n",
    "    knn_f1_metric = metrics.f1_score(y_dev, knn_f1_prediction, average='micro') * 100\n",
    "    knn_values.append(i)\n",
    "    knn_results.append(knn_f1_metric)\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(knn_values, knn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_values = []\n",
    "mnb_results = []\n",
    "for i in [0.000001, 0.00001, 0.0001, 0.0001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]:\n",
    "    mnb_f1 = MultinomialNB(alpha=i)\n",
    "    mnb_f1.fit(vecorized_train, y_train_use)\n",
    "    mnb_f1_prediction = mnb_f1.predict(vecorized_dev)\n",
    "    mnb_f1_metric = metrics.f1_score(y_dev, mnb_f1_prediction, average='micro') * 100\n",
    "    mnb_values.append(i)\n",
    "    mnb_results.append(mnb_f1_metric)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(mnb_values, mnb_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_values = []\n",
    "lr_results = []    \n",
    "for i in [0.000001, 0.00001, 0.0001, 0.0001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 20]:\n",
    "    lr_f1 = LogisticRegression(penalty='l2', C=i)\n",
    "    lr_f1.fit(vecorized_train, y_train_use)\n",
    "    lr_f1_prediction = lr_f1.predict(vecorized_dev)\n",
    "    lr_f1_metric = metrics.f1_score(y_dev, lr_f1_prediction, average='micro')  * 100\n",
    "    lr_values.append(i)\n",
    "    lr_results.append(lr_f1_metric)\n",
    "    \n",
    "fig = plt.figure()\n",
    "plt.plot(lr_values, lr_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_values = []\n",
    "rf_results = []  \n",
    "for i in range(1, 500, 50):\n",
    "    rf_f1 = RandomForestClassifier(n_estimators=i)\n",
    "    rf_f1.fit(vecorized_train, y_train_use)\n",
    "    rf_f1_prediction = rf_f1.predict(vecorized_dev)\n",
    "    rf_f1_metric = metrics.f1_score(y_dev, rf_f1_prediction, average='micro')  * 100\n",
    "    rf_values.append(i)\n",
    "    rf_results.append(rf_f1_metric)\n",
    "plt.plot(rf_values, rf_results)\n",
    "\n",
    "rf_f1 = RandomForestClassifier(n_estimators=50)\n",
    "rf_f1.fit(vecorized_train, y_train_use)\n",
    "rf_f1_prediction = rf_f1.predict(vecorized_dev)\n",
    "rf_f1_metric = metrics.f1_score(y_dev, rf_f1_prediction, average='micro')  * 100\n",
    "print 'rf_f1_metric', rf_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_f1 = DecisionTreeClassifier()\n",
    "dt_f1.fit(vecorized_train, y_train_use)\n",
    "dt_f1_prediction = dt_f1.predict(vecorized_dev)\n",
    "dt_f1_metric = metrics.f1_score(y_dev, dt_f1_prediction, average='micro')  * 100\n",
    "print 'dt_f1_metric', dt_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab_fit = AdaBoostClassifier(base_estimator=RandomForestClassifier(), n_estimators=600)\n",
    "ab_fit.fit(vecorized_train, y_train_use)\n",
    "ab_fit_prediction = ab_fit.predict(vecorized_dev)\n",
    "ab_fit_metric = metrics.f1_score(y_dev, ab_fit_prediction, average='micro')  * 100\n",
    "print 'ab_fit_metric', ab_fit_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_values = []\n",
    "et_results = []  \n",
    "for i in range(1, 500, 50):\n",
    "    et_f1 = RandomForestClassifier(n_estimators=i)\n",
    "    et_f1.fit(vecorized_train, y_train_use)\n",
    "    et_f1_prediction = et_f1.predict(vecorized_dev)\n",
    "    et_f1_metric = metrics.f1_score(y_dev, et_f1_prediction, average='micro')  * 100\n",
    "    et_values.append(i)\n",
    "    et_results.append(et_f1_metric)\n",
    "plt.plot(et_values, et_results)\n",
    "\n",
    "et_f1 = RandomForestClassifier(n_estimators=100)\n",
    "et_f1.fit(vecorized_train, y_train_use)\n",
    "et_f1_prediction = et_f1.predict(vecorized_dev)\n",
    "et_f1_metric = metrics.f1_score(y_dev, et_f1_prediction, average='micro')  * 100\n",
    "print 'et_f1_metric', et_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_fit = GradientBoostingClassifier()\n",
    "gb_fit.fit(vecorized_train.todense(), y_train_use)\n",
    "gb_fit_prediction = gb_fit.predict(vecorized_dev.todense())\n",
    "gb_fit_metric = metrics.f1_score(y_dev, gb_fit_prediction, average='micro')  * 100\n",
    "print 'gb_fit_metric', gb_fit_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_f1_prediction = lr_f1.predict(vecorized_test)\n",
    "lr_f1_metric = metrics.f1_score(y_test, lr_f1_prediction, average='micro')  * 100\n",
    "print 'lr_f1_metric', lr_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_f1_prediction = rf_f1.predict(vecorized_test)\n",
    "rf_f1_metric = metrics.f1_score(y_test, rf_f1_prediction, average='micro')  * 100\n",
    "print 'rf_f1_metric', rf_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_f1_prediction = dt_f1.predict(vecorized_test)\n",
    "dt_f1_metric = metrics.f1_score(y_test, dt_f1_prediction, average='micro')  * 100\n",
    "print 'dt_f1_metric', dt_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_f1_prediction = et_f1.predict(vecorized_test)\n",
    "et_f1_metric = metrics.f1_score(y_test, et_f1_prediction, average='micro')  * 100\n",
    "print 'et_f1_metric', et_f1_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ab_fit_prediction = ab_fit.predict(vecorized_test)\n",
    "ab_fit_metric = metrics.f1_score(y_test, ab_fit_prediction, average='micro')  * 100\n",
    "print 'ab_fit_metric', ab_fit_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb_fit_prediction = gb_fit.predict(vecorized_test.todense())\n",
    "gb_fit_metric = metrics.f1_score(y_test, gb_fit_prediction, average='micro')  * 100\n",
    "print 'gb_fit_metric', gb_fit_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "output_df = pd.DataFrame([lr_f1_prediction, rf_f1_prediction, dt_f1_prediction, ab_fit_prediction, et_f1_prediction, y_test]).transpose()\n",
    "output_df.columns = [\"lr_f1_prediction\", \"rf_f1_prediction\", \"dt_f1_prediction\", \"ab_fit_prediction\", \"et_f1_prediction\", \"y_test\"]\n",
    "output_df['Description'] = ''\n",
    "output_df['highest_mode'] = ''\n",
    "output_df['Description_highest_mode'] = ''\n",
    "output_df['text'] = ''\n",
    "\n",
    "itr = 0\n",
    "while itr < len(output_df):\n",
    "    \n",
    "    temp_mode = output_df.loc[itr, [\"lr_f1_prediction\", \"rf_f1_prediction\", \"dt_f1_prediction\", \"ab_fit_prediction\", \"et_f1_prediction\"]].mode()\n",
    "    temp_mode = temp_mode.get_values()[0]\n",
    "        \n",
    "    output_df.loc[itr, \"highest_mode\"] = temp_mode\n",
    "    output_df.loc[itr, \"y_test\"] = output_df.loc[itr, \"y_test\"].strip(\"]\")\n",
    "    try:\n",
    "        output_df.loc[itr, 'Description'] = master_df[master_df['NDB_NO'] == \"\\\"{}\\\"\".format(output_df.loc[itr, 'y_test'].strip('\"'))]['Description'].get_values()[0]\n",
    "    except:\n",
    "        print itr,\"FAILED\"\n",
    "    \n",
    "    \n",
    "    output_df.loc[itr, 'Description_highest_mode'] = master_df[master_df['NDB_NO'] == \"\\\"{}\\\"\".format(temp_mode.strip('\"'))]['Description'].get_values()[0]\n",
    "    \n",
    "#     print itr, X_test.iloc[itr]\n",
    "    \n",
    "    output_df.loc[itr, 'text'] = X_test.iloc[itr]\n",
    "    \n",
    "    itr += 1\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# output_df.to_csv(\"C:/Users/mgruz/Desktop/w210/data/recipe/check/output_prediction_ml.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
